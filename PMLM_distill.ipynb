{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d37ac-1497-4ee2-b464-ccb0c8e3c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert PMLM_distill.ipynb --to python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e6caf-cc74-492f-b108-82bb29644010",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Imports**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a2934-2b52-4d1c-8c55-df7c564ab54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# üí• Set this BEFORE model/accelerator is created\n",
    "os.environ[\"DEEPSPEED_USE_MPI\"] = \"false\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n",
    "os.environ[\"TORCH_NCCL_BLOCKING_WAIT\"] = \"1\"\n",
    "# os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel,\n",
    ")\n",
    "\n",
    "from peft.tuners.lora import LoraModel, LoraLayer\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from deepspeed.accelerator import get_accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d53ad7-3990-47c0-a90f-b6e9154ab824",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Installations**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30e1f7-684c-4890-a36d-dd28991ce6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --no-cache-dir --upgrade bitsandbytes triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551e08e-b33a-43ba-a4c2-aef457f69b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd19ca-27a8-4d26-8be5-ef59b1408698",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Config**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f2986-be12-4dd0-8693-71da0162e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9603bb-27d2-40b3-a649-3cbf1a40e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure HF_HOME is set explicitly before model download\n",
    "os.environ[\"HF_HOME\"] = \"../huggingface_cache\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"../huggingface_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55228d2c-70f3-4c70-9488-2437cc1efe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"INSERT_YOUR_OWN_TOKEN\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7792586-2244-4521-a2f5-1dc26548225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56725d7e-5d88-42d5-8073-b170283f8e9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Utils**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba563f-3afb-40c3-b371-b09c1da190a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(model_key: str, prompt: str) -> str:\n",
    "    \"\"\"Format prompt string based on model conventions for selected models only.\"\"\"\n",
    "\n",
    "    if \"gpt2\" in model_key:\n",
    "        return prompt  # plain input, no special formatting\n",
    "\n",
    "    elif \"mistral\" in model_key or \"ministral\" in model_key:\n",
    "        return f\"<s>[INST]{prompt}[/INST]\"\n",
    "\n",
    "    elif \"llama3\" in model_key:\n",
    "        return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{prompt}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "    elif \"qwen\" in model_key:\n",
    "        return f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\"\n",
    "\n",
    "    elif \"gemma\" in model_key:\n",
    "        return f\"<start_of_turn>user\\n{prompt}\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "    elif \"internlm\" in model_key:\n",
    "        return f\"<|User|>:{prompt}\\n<|Bot|>:\"\n",
    "\n",
    "    elif \"deepseek\" in model_key:\n",
    "        return f\"### Instruction:\\n{prompt}\\n### Response:\"\n",
    "\n",
    "    elif \"glm\" in model_key:\n",
    "        return f\"[Round 1]\\n\\nÈóÆÔºö{prompt}\\n\\nÁ≠îÔºö\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_key '{model_key}' in format_prompt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4172ca-4483-41d7-8fc2-f40712b111ca",
   "metadata": {},
   "source": [
    "**Hparams**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61ed53-b4ad-49a4-8918-43e15378b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "max_length = 512\n",
    "num_epochs = 1\n",
    "learning_rate = 2e-5\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "strategy_hint = (\n",
    "        \"\"\"\n",
    "        Generate a [SYSTEM INSTRUCTION] based on the provided [USER REQUEST]. This [SYSTEM INSTRUCTION] will be combined \n",
    "        with the [USER REQUEST] and input into another language model to produce a watermarked output. \n",
    "        The [SYSTEM INSTRUCTION] should specify watermarking strategies that adapt dynamically to the content of the [USER REQUEST].\n",
    "        Example [SYSTEM INSTRUCTION]: 'Use specific strategies to embed watermarks such as including special tokens or phrases that fit naturally with the content. The watermark should be later detectable by a classifier.'\n",
    "        Example watermarking strategies:\n",
    "        ‚Ä¢ Lexical Strategy: Incorporate specific rare or uncommon tokens as watermarks.\n",
    "        ‚Ä¢ Semantic Strategy: Embed semantically relevant but less common phrases.\n",
    "        ‚Ä¢ Structural Strategy: Modify sentence structure in subtle but detectable ways.\n",
    "        ‚Ä¢ <You can add Strategies if necessary>\n",
    "        Ensure watermarks are evenly distributed throughout the output.\n",
    "        Your task is to output ONLY the [SYSTEM INSTRUCTION] that specifies the concrete watermarking strategy.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Define the model names\n",
    "MODEL_NAMES = {\n",
    "    # Working PLM models\n",
    "    \"mistral_7b_v03_instruct\": \"mistralai/Mistral-7B-Instruct-v0.3\",  #‚úÖ Works\n",
    "    \n",
    "    # MLM models (teacher)\n",
    "    \"deepseek_llm_chat\": \"deepseek-ai/deepseek-llm-7b-chat\",  # ‚úÖ Works\n",
    "    \"qwen2.5_7b_instruct\": \"Qwen/Qwen2.5-7B-Instruct\",  # ‚úÖ Works\n",
    "    \"llama3_8b_instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",  # ‚úÖ Works\n",
    "    \"gemma_7b_it\": \"google/gemma-7b-it\",  # ‚úÖ Works\n",
    "    \"ministral_8b_instruct\": \"mistralai/Ministral-8B-Instruct-2410\",  #‚úÖ Works\n",
    "    \"glm_4_9b_chat\": \"THUDM/glm-4-9b-chat\",  # ‚úÖ Works\n",
    "    \"internlm2.5_7b_chat\": \"internlm/internlm2-chat-7b\",  # ‚úÖ Works\n",
    "    \n",
    "    # Student models\n",
    "    \"mistral_7b_v02_instruct\": \"mistralai/Mistral-7B-Instruct-v0.2\",  # ‚úÖ Works\n",
    "    \"qwen1.5_1.8b_instruct\": \"Qwen/Qwen1.5-1.8B-Chat\",  # ‚úÖ Works\n",
    "    \"tinyllama_1.1b_chat\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # ‚úÖ Works\n",
    "    \"gemma_1.1_2b_it\": \"google/gemma-1.1-2b-it\",  # ‚úÖ Works\n",
    "}\n",
    "\n",
    "\n",
    "# Tier\tModel\n",
    "# ‚≠ê Top-tier\tLLaMA-3 8B Instruct\n",
    "# ‚≠ê Top-tier\tQwen2.5-7B-Instruct\n",
    "# ‚≠ê Top-tier\tDeepSeek v2 7B-Chat\n",
    "# ‚≠ê Mid-tier\tMistral-7B-Instruct-v0.3\n",
    "# ‚≠ê Mid-tier\tGEMMA-7B-IT\n",
    "# ‚úÖ Bonus\tInternLM2.5-7B-Chat\n",
    "# ‚úÖ Bonus\tGLM-4-9B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23c557-d840-4578-9f33-11bfe98765ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLM_MODEL_KEY = \"mistral_7b_v03_instruct\"\n",
    "TEACHER_MODEL_KEY = \"mistral_7b_v03_instruct\"\n",
    "STUDENT_MODEL_KEY = \"mistral_7b_v02_instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9a87a-f0d6-427b-a73b-849a1c17638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial list:\n",
    "# ---\n",
    "# GPT3.5-turbo-0125\n",
    "# QWEN-plus\n",
    "# LLAMA3-8B\n",
    "# QWEN2.5-7B\n",
    "# QWEN2-1.5B\n",
    "# vicuna_7b_v1_3\n",
    "# vicuna_7b_v1_5\n",
    "# open_llama_3b\n",
    "# open_llama_7b\n",
    "# mistral_7b_v03\n",
    "# mistral_7b_v03_instruct\n",
    "# baize_v2_7b\n",
    "# GLM-4-plus\n",
    "# GLM-3-Turbo\n",
    "# LLAMA3-8B\n",
    "# GEMMA-7B\n",
    "# GPT4o-mini\n",
    "# GPT-4o\n",
    "# DEEPSEEK v2\n",
    "# CLAUDE-3.5-sonnet\n",
    "# INTERNLM2.5-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152b782-2977-4914-ad50-4e6a5dee753e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Deepspeed accelarate**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a39d09-0075-4f98-9e2a-0e1d5ed60f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import DeepSpeedPlugin\n",
    "\n",
    "deepspeed_plugin = DeepSpeedPlugin(\n",
    "    zero_stage=3,\n",
    "    offload_optimizer_device=\"cpu\",\n",
    "    offload_param_device=\"cpu\",\n",
    "    gradient_clipping=1.0\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=\"bf16\",\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    deepspeed_plugin=deepspeed_plugin\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cf95f-e86b-4ae4-950d-4e79fe7e106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable DeepSpeed zero3 if needed (for large models)\n",
    "if accelerator.state.deepspeed_plugin is not None:\n",
    "    print(\"DeepSpeed is enabled. Adjust configurations accordingly.\")\n",
    "else:\n",
    "    print(\"DeepSpeed is not being used.\") \n",
    "\n",
    "# Empty CUDA cache to free unused memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reduce fragmentation issues\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afccd95b-54a3-4b40-9026-743b40b2c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using {accelerator.num_processes} GPUs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5b072-e185-4395-b376-6ecc1de68d8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Dataset**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9482d6-4923-4ce6-8c1a-b86063445955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationDataset(Dataset):\n",
    "    def __init__(self, csv_dir, tokenizer):\n",
    "        self.data = self.load_all_csvs(csv_dir)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_all_csvs(self, csv_dir):\n",
    "        all_dfs = []\n",
    "        for file in os.listdir(csv_dir):\n",
    "            if file.endswith(\".csv\"):\n",
    "                df = pd.read_csv(os.path.join(csv_dir, file))\n",
    "                all_dfs.append(df)\n",
    "        if not all_dfs:\n",
    "            raise ValueError(f\"No CSVs found in: {csv_dir}\")\n",
    "        return pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.data.iloc[idx][\"USER REQUEST\"]\n",
    "        response = self.data.iloc[idx][\"NON-WATERMARKED RESPONSE\"]\n",
    "\n",
    "        full_text = prompt.strip() + \"\\n\" + response.strip()\n",
    "        tokenized = self.tokenizer(\n",
    "            full_text, padding=\"max_length\", truncation=True,\n",
    "            max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Mask out prompt tokens in labels\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            prompt_ids = self.tokenizer(prompt, truncation=True, max_length=self.max_length)[\"input_ids\"]\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[:len(prompt_ids)] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "def prepare_distillation_dataloader(csv_dir, tokenizer):\n",
    "    dataset = DistillationDataset(csv_dir, tokenizer)\n",
    "    # dataset = dataset.select(range(200)) # Testing\n",
    "    \n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad8875-a165-4c9e-8c5f-b02b774a8d16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Model Distillation**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7749fee-389f-4363-9009-b91b20046b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_with_lora(student_model, dataloader, accelerator):\n",
    "    student_model = prepare_model_for_kbit_training(student_model)\n",
    "    student_model.gradient_checkpointing_enable()\n",
    "    student_model.enable_input_require_grads()\n",
    "    student_model.config.use_cache = False\n",
    "\n",
    "    if hasattr(student_model.config, \"use_flash_attention_2\"):\n",
    "        student_model.config.use_flash_attention_2 = True\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "    )\n",
    "\n",
    "    student_model = get_peft_model(student_model, lora_config)\n",
    "    print(student_model.print_trainable_parameters())\n",
    "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=2e-5)\n",
    "\n",
    "    student_model, optimizer, dataloader = accelerator.prepare(student_model, optimizer, dataloader)\n",
    "    student_model = torch.compile(student_model)\n",
    "    student_model.train()\n",
    "    \n",
    "    best_loss = float(\"inf\")\n",
    "    patience = 2\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch = {key: value.to(accelerator.device, non_blocking=True) for key, value in batch.items()}\n",
    "            student_model.train(False)\n",
    "\n",
    "            outputs = student_model(**batch)\n",
    "            loss = outputs.loss / gradient_accumulation_steps\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if (step + 1) % 32 == 0 or step == len(dataloader) - 1:\n",
    "                optimizer.step()\n",
    "                get_accelerator().empty_cache()\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            if accelerator.is_main_process and (step + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} | Step {step + 1}/{len(dataloader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}. Best loss: {best_loss:.4f}\")\n",
    "                break\n",
    "                \n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb57121-d118-4429-90fe-6e6da88de910",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Training: Distillation**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164555a2-87c4-44ad-add1-a2479398b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKPOINT_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724845e-e4c8-4fe6-b762-c78e16fd3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_path = f\"tuned_models/distilled_{TEACHER_MODEL_KEY}_to_{STUDENT_MODEL_KEY}\"\n",
    "os.makedirs(distilled_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b31018-9d82-4d4d-bfa0-d672646e0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAMES[TEACHER_MODEL_KEY], trust_remote_code=True)\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAMES[STUDENT_MODEL_KEY], trust_remote_code=True)\n",
    "\n",
    "# Ensure a padding token exists\n",
    "if student_tokenizer.pad_token is None:\n",
    "    student_tokenizer.pad_token = student_tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAMES[STUDENT_MODEL_KEY],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=None,\n",
    "    low_cpu_mem_usage=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6831c-3937-4f8f-8ecc-39253e8478e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = f\"Datasets/10k/PLM_{PLM_MODEL_KEY}/{TEACHER_MODEL_KEY}_data/\"\n",
    "distillation_dataloader = prepare_distillation_dataloader(csv_dir, student_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646f45b-80aa-466c-a12a-890eba407369",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_model = distill_with_lora(student_model, distillation_dataloader, accelerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95768fc2-6154-44ee-8942-7be25e945d36",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Saving**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff25d2-377c-438b-844c-ce4e31e0a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save just LoRA adapter weights\n",
    "if accelerator.is_main_process:\n",
    "    accelerator.unwrap_model(distilled_model).save_pretrained(distilled_path)\n",
    "    student_tokenizer.save_pretrained(distilled_path)\n",
    "    print(f\"Distilled model saved to: {distilled_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cent",
   "language": "python",
   "name": "torch_cent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
