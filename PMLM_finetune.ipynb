{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d37ac-1497-4ee2-b464-ccb0c8e3c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert PMLM_finetune.ipynb --to python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e6caf-cc74-492f-b108-82bb29644010",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Imports**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a2934-2b52-4d1c-8c55-df7c564ab54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# üí• Set this BEFORE model/accelerator is created\n",
    "os.environ[\"DEEPSPEED_USE_MPI\"] = \"false\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n",
    "os.environ[\"TORCH_NCCL_BLOCKING_WAIT\"] = \"1\"\n",
    "# os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel,\n",
    ")\n",
    "\n",
    "from peft.tuners.lora import LoraModel, LoraLayer\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from deepspeed.accelerator import get_accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d53ad7-3990-47c0-a90f-b6e9154ab824",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Installations**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30e1f7-684c-4890-a36d-dd28991ce6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --no-cache-dir --upgrade bitsandbytes triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551e08e-b33a-43ba-a4c2-aef457f69b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd19ca-27a8-4d26-8be5-ef59b1408698",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Config**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f2986-be12-4dd0-8693-71da0162e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9603bb-27d2-40b3-a649-3cbf1a40e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure HF_HOME is set explicitly before model download\n",
    "os.environ[\"HF_HOME\"] = \"../huggingface_cache\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"../huggingface_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55228d2c-70f3-4c70-9488-2437cc1efe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"INSERT_YOUR_OWN_TOKEN\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7792586-2244-4521-a2f5-1dc26548225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56725d7e-5d88-42d5-8073-b170283f8e9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Utils**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba563f-3afb-40c3-b371-b09c1da190a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(model_key: str, prompt: str) -> str:\n",
    "    \"\"\"Format prompt string based on model conventions for selected models only.\"\"\"\n",
    "\n",
    "    if \"gpt2\" in model_key:\n",
    "        return prompt  # plain input, no special formatting\n",
    "\n",
    "    elif \"mistral\" in model_key or \"ministral\" in model_key:\n",
    "        return f\"<s>[INST]{prompt}[/INST]\"\n",
    "\n",
    "    elif \"llama3\" in model_key:\n",
    "        return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{prompt}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "    elif \"qwen\" in model_key:\n",
    "        return f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\"\n",
    "\n",
    "    elif \"gemma\" in model_key:\n",
    "        return f\"<start_of_turn>user\\n{prompt}\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "    elif \"internlm\" in model_key:\n",
    "        return f\"<|User|>:{prompt}\\n<|Bot|>:\"\n",
    "\n",
    "    elif \"deepseek\" in model_key:\n",
    "        return f\"### Instruction:\\n{prompt}\\n### Response:\"\n",
    "\n",
    "    elif \"glm\" in model_key:\n",
    "        return f\"[Round 1]\\n\\nÈóÆÔºö{prompt}\\n\\nÁ≠îÔºö\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_key '{model_key}' in format_prompt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4172ca-4483-41d7-8fc2-f40712b111ca",
   "metadata": {},
   "source": [
    "**Hparams**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61ed53-b4ad-49a4-8918-43e15378b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "max_length = 512\n",
    "num_epochs = 1\n",
    "learning_rate = 2e-5\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "strategy_hint = (\n",
    "        \"\"\"\n",
    "        Generate a [SYSTEM INSTRUCTION] based on the provided [USER REQUEST]. This [SYSTEM INSTRUCTION] will be combined \n",
    "        with the [USER REQUEST] and input into another language model to produce a watermarked output. \n",
    "        The [SYSTEM INSTRUCTION] should specify watermarking strategies that adapt dynamically to the content of the [USER REQUEST].\n",
    "        Example [SYSTEM INSTRUCTION]: 'Use specific strategies to embed watermarks such as including special tokens or phrases that fit naturally with the content. The watermark should be later detectable by a classifier.'\n",
    "        Example watermarking strategies:\n",
    "        ‚Ä¢ Lexical Strategy: Incorporate specific rare or uncommon tokens as watermarks.\n",
    "        ‚Ä¢ Semantic Strategy: Embed semantically relevant but less common phrases.\n",
    "        ‚Ä¢ Structural Strategy: Modify sentence structure in subtle but detectable ways.\n",
    "        ‚Ä¢ <You can add Strategies if necessary>\n",
    "        Ensure watermarks are evenly distributed throughout the output.\n",
    "        Your task is to output ONLY the [SYSTEM INSTRUCTION] that specifies the concrete watermarking strategy.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Define the model names\n",
    "TEACHER_MODEL_NAMES = {\n",
    "    # Working PLM models\n",
    "    \"mistral_7b_v03_instruct\": \"mistralai/Mistral-7B-Instruct-v0.3\",  #‚úÖ Works\n",
    "    \n",
    "    # MLM models (teacher)\n",
    "    \"deepseek_llm_chat\": \"deepseek-ai/deepseek-llm-7b-chat\",  # ‚úÖ Works\n",
    "    \"qwen2.5_7b_instruct\": \"Qwen/Qwen2.5-7B-Instruct\",  # ‚úÖ Works\n",
    "    \"llama3_8b_instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",  # ‚úÖ Works\n",
    "    \"gemma_7b_it\": \"google/gemma-7b-it\",  # ‚úÖ Works\n",
    "    \"ministral_8b_instruct\": \"mistralai/Ministral-8B-Instruct-2410\",  #‚úÖ Works\n",
    "    \"glm_4_9b_chat\": \"THUDM/glm-4-9b-chat\",  # ‚úÖ Works\n",
    "    \"internlm2.5_7b_chat\": \"internlm/internlm2-chat-7b\",  # ‚úÖ Works\n",
    "}\n",
    "\n",
    "# Tier\tModel\n",
    "# ‚≠ê Top-tier\tLLaMA-3 8B Instruct\n",
    "# ‚≠ê Top-tier\tQwen2.5-7B-Instruct\n",
    "# ‚≠ê Top-tier\tDeepSeek v2 7B-Chat\n",
    "# ‚≠ê Mid-tier\tMistral-7B-Instruct-v0.3\n",
    "# ‚≠ê Mid-tier\tGEMMA-7B-IT\n",
    "# ‚úÖ Bonus\tInternLM2.5-7B-Chat\n",
    "# ‚úÖ Bonus\tGLM-4-9B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23c557-d840-4578-9f33-11bfe98765ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_MODEL_KEY = \"mistral_7b_v03_instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9a87a-f0d6-427b-a73b-849a1c17638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial list:\n",
    "# ---\n",
    "# GPT3.5-turbo-0125\n",
    "# QWEN-plus\n",
    "# LLAMA3-8B\n",
    "# QWEN2.5-7B\n",
    "# QWEN2-1.5B\n",
    "# vicuna_7b_v1_3\n",
    "# vicuna_7b_v1_5\n",
    "# open_llama_3b\n",
    "# open_llama_7b\n",
    "# mistral_7b_v03\n",
    "# mistral_7b_v03_instruct\n",
    "# baize_v2_7b\n",
    "# GLM-4-plus\n",
    "# GLM-3-Turbo\n",
    "# LLAMA3-8B\n",
    "# GEMMA-7B\n",
    "# GPT4o-mini\n",
    "# GPT-4o\n",
    "# DEEPSEEK v2\n",
    "# CLAUDE-3.5-sonnet\n",
    "# INTERNLM2.5-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5b072-e185-4395-b376-6ecc1de68d8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Dataset**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7054d9-728d-49ea-b212-16db7de14f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFtoTorchDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        return {key: torch.tensor(value) for key, value in example.items()}\n",
    "\n",
    "def prepare_alpaca_dataset_fine_tuning(teacher_tokenizer, model_key, dataset_name=\"tatsu-lab/alpaca\"):\n",
    "    \"\"\"\n",
    "    Prepares the Alpaca dataset for fine-tuning with correct prompt formatting.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "    def preprocess_fine_tuning(example):\n",
    "        raw_prompt = f\"{example['instruction']}\\n{example['input']}\" if example['input'] else example['instruction']\n",
    "        formatted_prompt = format_prompt(model_key, raw_prompt)  # ‚úÖ Apply model-specific formatting\n",
    "        response = example[\"output\"]\n",
    "\n",
    "        full_text = formatted_prompt + response  # ‚úÖ No extra newline, already handled by format_prompt\n",
    "\n",
    "        tokenized = teacher_tokenizer(\n",
    "            full_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        prompt_tokens = teacher_tokenizer(\n",
    "            formatted_prompt, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        prompt_len = prompt_tokens[\"input_ids\"].squeeze(0).ne(teacher_tokenizer.pad_token_id).sum().item()\n",
    "        labels[:prompt_len] = -100  # ‚úÖ Mask out prompt tokens\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    \n",
    "    # dataset = dataset.select(range(200)) # Testing\n",
    "    \n",
    "    dataset = dataset.map(preprocess_fine_tuning, batched=False, remove_columns=dataset.column_names)\n",
    "    torch_dataset = HFtoTorchDataset(dataset)\n",
    "    return DataLoader(torch_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c7b1c-f686-49af-951b-c0d88bafcf3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Fine-Tuning**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f36a0b-9562-4ca6-b3a6-64038df68e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_with_lora(teacher_model, dataloader, optimizer, accelerator):\n",
    "    teacher_model, optimizer, dataloader = accelerator.prepare(teacher_model, optimizer, dataloader)\n",
    "    teacher_model = torch.compile(teacher_model)\n",
    "    teacher_model.train()\n",
    "    \n",
    "    best_loss = float(\"inf\")\n",
    "    patience = 2\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch = {key: value.to(accelerator.device, non_blocking=True) for key, value in batch.items()}\n",
    "            outputs = teacher_model(**batch)\n",
    "            loss = outputs.loss / gradient_accumulation_steps\n",
    "            total_loss += loss.detach().item()\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if (step + 1) % 32 == 0 or step == len(dataloader) - 1:\n",
    "                optimizer.step()\n",
    "                get_accelerator().empty_cache()\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            if accelerator.is_main_process and (step + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} | Step {step + 1}/{len(dataloader)} | Loss: {loss.item():.4f}\")\n",
    "                \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}. Best loss: {best_loss:.4f}\")\n",
    "                break\n",
    "                \n",
    "    return teacher_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88441faf-69bd-4fe8-a3a2-776b0c2f9ac0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Model and optimizer**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522934f9-ae60-40aa-8de1-ff58ade7dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAMES[TEACHER_MODEL_KEY], trust_remote_code=True)\n",
    "\n",
    "# Ensure a padding token exists\n",
    "if teacher_tokenizer.pad_token is None:\n",
    "    teacher_tokenizer.pad_token = teacher_tokenizer.eos_token\n",
    "\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    TEACHER_MODEL_NAMES[TEACHER_MODEL_KEY],\n",
    "    cache_dir=os.environ[\"HF_HOME\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None,\n",
    "    low_cpu_mem_usage=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "teacher_model = prepare_model_for_kbit_training(teacher_model)\n",
    "teacher_model.gradient_checkpointing_enable()\n",
    "teacher_model.enable_input_require_grads()\n",
    "teacher_model.config.use_cache = False\n",
    "\n",
    "if hasattr(teacher_model.config, \"use_flash_attention_2\"):\n",
    "    teacher_model.config.use_flash_attention_2 = True\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "teacher_model = get_peft_model(teacher_model, lora_config)\n",
    "print(teacher_model.print_trainable_parameters())\n",
    "optimizer = torch.optim.AdamW(teacher_model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829de9c8-defa-4fa5-b301-7071cf1559e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Deepspeed accelarate**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a39d09-0075-4f98-9e2a-0e1d5ed60f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import DeepSpeedPlugin\n",
    "\n",
    "deepspeed_plugin = DeepSpeedPlugin(\n",
    "    zero_stage=3,\n",
    "    offload_optimizer_device=\"cpu\",\n",
    "    offload_param_device=\"cpu\",\n",
    "    gradient_clipping=1.0\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=\"bf16\",\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    deepspeed_plugin=deepspeed_plugin\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cf95f-e86b-4ae4-950d-4e79fe7e106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable DeepSpeed zero3 if needed (for large models)\n",
    "if accelerator.state.deepspeed_plugin is not None:\n",
    "    print(\"DeepSpeed is enabled. Adjust configurations accordingly.\")\n",
    "else:\n",
    "    print(\"DeepSpeed is not being used.\") \n",
    "\n",
    "# Empty CUDA cache to free unused memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reduce fragmentation issues\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afccd95b-54a3-4b40-9026-743b40b2c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using {accelerator.num_processes} GPUs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c419b7-fee4-4e24-88a7-058532b32281",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Training: Fine-Tuning**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0cfa90-0c1f-46ff-8009-711f8c4bce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKPOINT_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d73e3-bc55-476d-bfce-86bb88b4629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_path = f\"tuned_models/fine_tuned_{TEACHER_MODEL_KEY}\"\n",
    "os.makedirs(fine_tuned_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ffb297-1b8e-40f6-9735-6a9df0b10a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_dataloader = prepare_alpaca_dataset_fine_tuning(teacher_tokenizer, TEACHER_MODEL_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3350b4-05be-42c5-bf29-138e27ffb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = fine_tune_with_lora(teacher_model, fine_tuning_dataloader, optimizer, accelerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ccb30-c107-4295-920b-27bdfef658dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Saving**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132f8c3-276c-49e7-af73-081d93e306ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save just LoRA adapter weights\n",
    "if accelerator.is_main_process:\n",
    "    accelerator.unwrap_model(fine_tuned_model).save_pretrained(fine_tuned_path)\n",
    "    teacher_tokenizer.save_pretrained(fine_tuned_path)\n",
    "    print(f\"Fine-tuned model saved to: {fine_tuned_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cent",
   "language": "python",
   "name": "torch_cent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
