# Watermarking Language Models through Language Models
---

**üîç Project Overview**
---
This repository presents our approach to watermarking language model outputs using dynamically constructed system instructions generated by the Prompting LM. The figure below illustrates our full pipeline, highlighting the interaction between user requests, system instructions, the Marking LM, and the Detecting LM.

**Overview of the proposed scheme:**
---
![overview](https://github.com/user-attachments/assets/6f26dbfa-5cbe-412d-9953-5d1076e4bba8)


**üí¨ Fixed Prompt for the Prompting Language Model:**
---
The fixed prompt used to guide the Prompting LM in generating system instructions is shown below. This template ensures consistent instruction generation while allowing for diverse lexical, semantic, or structural watermarking strategies.
![fixed_p](https://github.com/user-attachments/assets/31aecde7-c20e-44fa-8fa0-d775640500fb)

**üìä Results Snapshot:**
---
The following screenshot summarizes our main quantitative findings, including detection accuracy under fine-tuning and distillation. Our method achieves high robustness even under model transformations, demonstrating the reliability and adaptability of the watermark signal.
![Capture](https://github.com/user-attachments/assets/8b1a7855-e797-427f-8cdb-3370eb6b3e9f)

**üìö Citation:**
---
```bibtex
@ARTICLE{11146861,
  author={Dasgupta, Agnibh and Tanvir, Abdullah All and Zhong, Xin},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Watermarking Language Models through Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  keywords={Watermarking;Adaptation models;Robustness;Training;Large language models;Intellectual property;Closed box;Tuning;Context modeling;Codes;Content authentication;instruction control;large language models;prompt engineering;robust watermarking},
  doi={10.1109/TAI.2025.3605117}
}
```
üìå A. Dasgupta, A. A. Tanvir and X. Zhong, "Watermarking Language Models through Language Models," in IEEE Transactions on Artificial Intelligence, doi: 10.1109/TAI.2025.3605117.
