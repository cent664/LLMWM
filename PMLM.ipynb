{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939b837-11a3-4797-af5d-42e42fedf410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !jupyter nbconvert PMLM.ipynb --to python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e6caf-cc74-492f-b108-82bb29644010",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Imports**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a2934-2b52-4d1c-8c55-df7c564ab54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import Union, List\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList, MinLengthLogitsProcessor\n",
    "from peft import PeftModel\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56b328-e0f4-4a3e-9a67-80604b014733",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Installations**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e283ab-e657-4c64-9516-9dc55402035f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd19ca-27a8-4d26-8be5-ef59b1408698",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Config**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9603bb-27d2-40b3-a649-3cbf1a40e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure HF_HOME is set explicitly before model download\n",
    "os.environ[\"HF_HOME\"] = \"../huggingface_cache\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"../huggingface_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55228d2c-70f3-4c70-9488-2437cc1efe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"INSERT_YOUR_OWN_TOKEN\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7792586-2244-4521-a2f5-1dc26548225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806ab6d-d027-47c8-b46a-1b923f7b2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "# Scan the Hugging Face cache\n",
    "cache_info = scan_cache_dir(\"../huggingface_cache\")\n",
    "\n",
    "# Print cache details\n",
    "print(f\"Cache size: {cache_info.size_on_disk / (1024**3):.2f} GB\")  # Convert bytes to GB\n",
    "print(\"Cached Repositories:\")\n",
    "for repo in cache_info.repos:\n",
    "    print(f\"- {repo.repo_id} ({repo.repo_type}): {repo.size_on_disk / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56725d7e-5d88-42d5-8073-b170283f8e9e",
   "metadata": {},
   "source": [
    "**Hparams**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61ed53-b4ad-49a4-8918-43e15378b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "USING_MAX_DATASET = False\n",
    "output_token_limit = 2048\n",
    "\n",
    "if USING_MAX_DATASET:\n",
    "    custom_prompts_data_path = \"Datasets/10k_category_prompts.csv\"\n",
    "else:\n",
    "    custom_prompts_data_path = \"Datasets/1k_category_prompts.csv\"\n",
    "    \n",
    "strategy_hint = (\n",
    "        \"\"\"\n",
    "        Generate a [SYSTEM INSTRUCTION] based on the provided [USER REQUEST]. This [SYSTEM INSTRUCTION] will be combined \n",
    "        with the [USER REQUEST] and input into another language model to produce a watermarked output. \n",
    "        The [SYSTEM INSTRUCTION] should specify watermarking strategies that adapt dynamically to the content of the [USER REQUEST].\n",
    "        Example [SYSTEM INSTRUCTION]: 'Use specific strategies to embed watermarks such as including special tokens or phrases that fit naturally with the content. The watermark should be later detectable by a classifier.'\n",
    "        Example watermarking strategies:\n",
    "        â€¢ Lexical Strategy: Incorporate specific rare or uncommon tokens as watermarks.\n",
    "        â€¢ Semantic Strategy: Embed semantically relevant but less common phrases.\n",
    "        â€¢ Structural Strategy: Modify sentence structure in subtle but detectable ways.\n",
    "        â€¢ <You can add Strategies if necessary>\n",
    "        Ensure watermarks are evenly distributed throughout the output.\n",
    "        Your task is to output ONLY the [SYSTEM INSTRUCTION] that specifies the concrete watermarking strategy.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Define the model names\n",
    "MODEL_NAMES = {\n",
    "    # Working PLM models\n",
    "    \"mistral_7b_v03_instruct\": \"mistralai/Mistral-7B-Instruct-v0.3\",  #âœ… Works\n",
    "    \n",
    "    # MLM models (teacher)\n",
    "    \"deepseek_llm_chat\": \"deepseek-ai/deepseek-llm-7b-chat\",  # âœ… Works\n",
    "    \"qwen2.5_7b_instruct\": \"Qwen/Qwen2.5-7B-Instruct\",  # âœ… Works\n",
    "    \"llama3_8b_instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",  # âœ… Works\n",
    "    \"gemma_7b_it\": \"google/gemma-7b-it\",  # âœ… Works\n",
    "    \"ministral_8b_instruct\": \"mistralai/Ministral-8B-Instruct-2410\",  #âœ… Works\n",
    "    \"glm_4_9b_chat\": \"THUDM/glm-4-9b-chat\",  # âœ… Works\n",
    "    \"internlm2.5_7b_chat\": \"internlm/internlm2-chat-7b\",  # âœ… Works\n",
    "\n",
    "    \"gpt4o: manual\"\n",
    "    \n",
    "    # Student models\n",
    "    \"mistral_7b_v02_instruct\": \"mistralai/Mistral-7B-Instruct-v0.2\",  # âœ… Works\n",
    "    \"qwen1.5_1.8b_instruct\": \"Qwen/Qwen1.5-1.8B-Chat\",  # âœ… Works\n",
    "    \"tinyllama_1.1b_chat\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # âœ… Works\n",
    "    \"gemma_1.1_2b_it\": \"google/gemma-1.1-2b-it\",  # âœ… Works\n",
    "}\n",
    "\n",
    "# Tier\tModel\n",
    "# â­ Top-tier\tLLaMA-3 8B Instruct\n",
    "# â­ Top-tier\tQwen2.5-7B-Instruct\n",
    "# â­ Top-tier\tDeepSeek v2 7B-Chat\n",
    "# â­ Mid-tier\tMistral-7B-Instruct-v0.3\n",
    "# â­ Mid-tier\tGEMMA-7B-IT\n",
    "# âœ… Bonus\tInternLM2.5-7B-Chat\n",
    "# âœ… Bonus\tGLM-4-9B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f462d5-7fdf-494d-93e0-51b79f47d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial list:\n",
    "# ---\n",
    "# GPT3.5-turbo-0125\n",
    "# QWEN-plus\n",
    "# LLAMA3-8B\n",
    "# QWEN2.5-7B\n",
    "# QWEN2-1.5B\n",
    "# vicuna_7b_v1_3\n",
    "# vicuna_7b_v1_5\n",
    "# open_llama_3b\n",
    "# open_llama_7b\n",
    "# mistral_7b_v03\n",
    "# mistral_7b_v03_instruct\n",
    "# baize_v2_7b\n",
    "# GLM-4-plus\n",
    "# GLM-3-Turbo\n",
    "# LLAMA3-8B\n",
    "# GEMMA-7B\n",
    "# GPT4o-mini\n",
    "# GPT-4o\n",
    "# DEEPSEEK v2\n",
    "# CLAUDE-3.5-sonnet\n",
    "# INTERNLM2.5-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb213dd-b9be-49e3-b94d-dfd7cd432c56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Special case for ablation**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a51a6-8d5f-473d-8442-98983e200802",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ablation = True\n",
    "experiment = 'structural'\n",
    "\n",
    "if experiment == 'lexical':\n",
    "    strategy_hint = (\n",
    "        \"\"\"\n",
    "        Generate a [SYSTEM INSTRUCTION] based on the provided [USER REQUEST]. This [SYSTEM INSTRUCTION] will be combined \n",
    "        with the [USER REQUEST] and input into another language model to produce a watermarked output. \n",
    "        The [SYSTEM INSTRUCTION] should specify watermarking strategies that adapt dynamically to the content of the [USER REQUEST].\n",
    "        Example [SYSTEM INSTRUCTION]: 'Use specific strategies to embed watermarks such as including special tokens or phrases that fit naturally with the content. The watermark should be later detectable by a classifier.'\n",
    "        Watermarking strategy:\n",
    "        â€¢ Lexical Strategy: Incorporate specific rare or uncommon tokens as watermarks.\n",
    "        Ensure watermarks are evenly distributed throughout the output.\n",
    "        Your task is to output ONLY the [SYSTEM INSTRUCTION] that specifies the concrete watermarking strategy.\n",
    "        \"\"\"\n",
    "    )\n",
    "elif experiment == 'semantic':\n",
    "    strategy_hint = (\n",
    "        \"\"\"\n",
    "        Generate a [SYSTEM INSTRUCTION] based on the provided [USER REQUEST]. This [SYSTEM INSTRUCTION] will be combined \n",
    "        with the [USER REQUEST] and input into another language model to produce a watermarked output. \n",
    "        The [SYSTEM INSTRUCTION] should specify watermarking strategies that adapt dynamically to the content of the [USER REQUEST].\n",
    "        Example [SYSTEM INSTRUCTION]: 'Use specific strategies to embed watermarks such as including special tokens or phrases that fit naturally with the content. The watermark should be later detectable by a classifier.'\n",
    "        Watermarking strategy:\n",
    "        â€¢ Semantic Strategy: Embed semantically relevant but less common phrases.\n",
    "        Ensure watermarks are evenly distributed throughout the output.\n",
    "        Your task is to output ONLY the [SYSTEM INSTRUCTION] that specifies the concrete watermarking strategy.\n",
    "        \"\"\"\n",
    "    )\n",
    "elif experiment == 'structural':\n",
    "    strategy_hint = (\n",
    "        \"\"\"\n",
    "        Generate a [SYSTEM INSTRUCTION] based on the provided [USER REQUEST]. This [SYSTEM INSTRUCTION] will be combined \n",
    "        with the [USER REQUEST] and input into another language model to produce a watermarked output. \n",
    "        The [SYSTEM INSTRUCTION] should specify watermarking strategies that adapt dynamically to the content of the [USER REQUEST].\n",
    "        Example [SYSTEM INSTRUCTION]: 'Use specific strategies to embed watermarks such as including special tokens or phrases that fit naturally with the content. The watermark should be later detectable by a classifier.'\n",
    "        Watermarking strategy:\n",
    "        â€¢ Structural Strategy: Modify sentence structure in subtle but detectable ways.\n",
    "        Ensure watermarks are evenly distributed throughout the output.\n",
    "        Your task is to output ONLY the [SYSTEM INSTRUCTION] that specifies the concrete watermarking strategy.\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4065262-5be1-4c2a-8f5b-da74080707f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_ablation:\n",
    "    custom_prompts_data_path = \"Datasets/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5b072-e185-4395-b376-6ecc1de68d8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Dataset**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0bea6-be6e-41e0-b99c-237dd20d39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom TextDataset class to handle both prompt and category\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, requests):\n",
    "        self.requests = requests  # Each item in requests is a tuple (prompt, category)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.requests)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt, category = self.requests[idx]\n",
    "        return {\"prompt\": prompt, \"category\": category}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f61a4e-fbb2-49a1-9087-b6ea6739456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to prepare DataLoader\n",
    "def prepare_dataset(requests, batch_size=batch_size):\n",
    "    dataset = TextDataset(requests)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 1: Load additional prompts from the CSV file\n",
    "csv_prompts = pd.read_csv(custom_prompts_data_path)  # Replace with your file path\n",
    "\n",
    "# Step 2: Extract prompts and their corresponding categories\n",
    "user_requests = csv_prompts['prompt'].tolist()\n",
    "categories = csv_prompts['category'].tolist()\n",
    "\n",
    "# Step 3: Combine prompts and categories into tuples for the dataset\n",
    "all_user_requests = list(zip(user_requests, categories))\n",
    "\n",
    "# Step 4: Prepare the DataLoader with the CSV-only dataset\n",
    "dataloader = prepare_dataset(all_user_requests, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f51061-ab47-4e98-b3ef-5fafa1b6e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cb3fb-e10f-4482-93db-9f7c429aaec2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Prompt engineering**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f9792-0f62-4b58-bb53-947f9888fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(model_key: str, prompt: str) -> str:\n",
    "    \"\"\"Format prompt string based on model conventions for selected models only.\"\"\"\n",
    "\n",
    "    if \"gpt\" in model_key:\n",
    "        return prompt\n",
    "    \n",
    "    elif \"llama3\" in model_key:\n",
    "        return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{prompt}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "    elif \"tinyllama\" in model_key:\n",
    "        return f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "\n",
    "    elif \"mistral\" in model_key or \"ministral\" in model_key:\n",
    "        return f\"<s>[INST]{prompt}[/INST]\"\n",
    "\n",
    "    elif \"qwen\" in model_key:\n",
    "        return f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\"\n",
    "\n",
    "    elif \"gemma\" in model_key:\n",
    "        return f\"<start_of_turn>user\\n{prompt}\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "    elif \"internlm\" in model_key:\n",
    "        return f\"<|User|>:{prompt}\\n<|Bot|>:\"\n",
    "\n",
    "    elif \"deepseek\" in model_key:\n",
    "        return f\"### Instruction:\\n{prompt}\\n### Response:\"\n",
    "\n",
    "    elif \"glm\" in model_key:\n",
    "        return f\"[Round 1]\\n\\né—®ï¼š{prompt}\\n\\nç­”ï¼š\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_key '{model_key}' in format_prompt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa6912-878f-4593-99e2-293740eccacd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Loading model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e6d6e-40ea-44df-98c4-4c9363c03d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_key: str, modified_model_path: str = None):\n",
    "    if model_key not in MODEL_NAMES:\n",
    "        raise ValueError(f\"Invalid model key '{model_key}'. Choose from: {list(MODEL_NAMES.keys())}\")\n",
    "    \n",
    "    # Tokenizer\n",
    "    model_name = MODEL_NAMES[model_key]\n",
    "    tokenizer_path = modified_model_path if modified_model_path else model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_path,\n",
    "        cache_dir=os.environ[\"HF_HOME\"],\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"left\"\n",
    "    )\n",
    "    \n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    \n",
    "    # Modified model\n",
    "    if modified_model_path:\n",
    "        print(f\"Loading fine-tuned model from {modified_model_path}...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=os.environ[\"HF_HOME\"],\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Safely load LoRA\n",
    "        model = PeftModel.from_pretrained(base_model, modified_model_path)\n",
    "\n",
    "        try:\n",
    "            model = model.merge_and_unload()\n",
    "            print(f\"Successfully merged LoRA and base model!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: merge_and_unload failed, using adapters dynamically. {e}\")\n",
    "\n",
    "    # Base model\n",
    "    else:\n",
    "        print(f\"Loading base model: {model_name}...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=os.environ[\"HF_HOME\"],\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "    # Freeze model for inference\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b518e-7d0e-45bb-9690-baba4ac37deb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Query**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50313c0b-747b-4c98-a867-230495b8e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopOnNthString(StoppingCriteria):\n",
    "    def __init__(self, stop_strings, tokenizer, occurrence=1):\n",
    "        self.stop_strings = stop_strings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.occurrence = occurrence\n",
    "        self.counter = {s: 0 for s in stop_strings}\n",
    "        self.buffer = \"\"\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Only decode the last 20 tokens to avoid full decode\n",
    "        last_ids = input_ids[0, -20:].tolist()\n",
    "        new_text = self.tokenizer.decode(last_ids, skip_special_tokens=True)\n",
    "        self.buffer += new_text\n",
    "\n",
    "        for stop_str in self.stop_strings:\n",
    "            self.counter[stop_str] = self.buffer.count(stop_str)\n",
    "            if self.counter[stop_str] >= self.occurrence:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def custom_stopping_criteria_for_strings(stop_strings, tokenizer, occurrence=1):\n",
    "    return StoppingCriteriaList([StopOnNthString(stop_strings, tokenizer, occurrence=occurrence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be291db4-6e2e-4477-a788-9471f3ca0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(model_key: str, tokenizer, model, chat_prompts: Union[str, List[str]]) -> Union[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Generates responses from the selected model for a single or batch of prompts.\n",
    "    \"\"\"\n",
    "    is_single = isinstance(chat_prompts, str)\n",
    "    if is_single:\n",
    "        chat_prompts = [chat_prompts]\n",
    "\n",
    "    formatted_prompts = [format_prompt(model_key, p) for p in chat_prompts]\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    eos_token_id = torch.tensor([tokenizer.eos_token_id], device=model.device)\n",
    "\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": output_token_limit,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.7,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"use_cache\": True,\n",
    "    }\n",
    "    # ðŸ”¥ SPECIAL CASE PATCHING\n",
    "    if any(m in model_key for m in [\"internlm\", \"glm_4\"]):\n",
    "        print(f\"Note: For {model_key}, forcing safe generation parameters.\")\n",
    "        generation_kwargs[\"do_sample\"] = False\n",
    "        generation_kwargs[\"use_cache\"] = False\n",
    "        generation_kwargs[\"temperature\"] = None\n",
    "        generation_kwargs[\"top_p\"] = None\n",
    "        generation_kwargs[\"max_new_tokens\"] = 512\n",
    "        \n",
    "    # GLM: Stop on FIRST [Round 2]\n",
    "    if any(m in model_key for m in [\"glm_4\", \"glm_3\"]):\n",
    "        generation_kwargs[\"stopping_criteria\"] = custom_stopping_criteria_for_strings(\n",
    "            [\"[Round 2]\"], tokenizer, occurrence=1\n",
    "        )\n",
    "\n",
    "    # InternLM: Stop on SECOND <|User|>:\n",
    "    if \"internlm\" in model_key:\n",
    "        generation_kwargs[\"stopping_criteria\"] = custom_stopping_criteria_for_strings(\n",
    "            [\"<|User|>:\"], tokenizer, occurrence=2\n",
    "        )\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **generation_kwargs)\n",
    "\n",
    "    responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return responses[0] if is_single else responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9f8cb-2e04-4e36-b003-cabcbe2268f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Cleaning**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b3c98-876f-4c52-aa52-45e8fccdbd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(model_key: str, prompts: Union[str, List[str]], responses: Union[str, List[str]]) -> Union[str, List[str]]:\n",
    "    \"\"\"Cleans model responses based on prompt and model_key.\"\"\"\n",
    "\n",
    "    def clean_pair(prompt: str, response: str) -> str:\n",
    "        prompt = prompt.strip()\n",
    "        response = response.strip()\n",
    "\n",
    "        if \"gpt\" in model_key:\n",
    "            return response  # raw output, no cleaning\n",
    "        \n",
    "        elif \"llama3\" in model_key:\n",
    "            if \"assistant\" in response:\n",
    "                response = response.split(\"assistant\", 1)[-1].strip()\n",
    "            return response\n",
    "\n",
    "        elif \"tinyllama\" in model_key:\n",
    "            if \"<|assistant|>\" in response:\n",
    "                response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "            return response\n",
    "\n",
    "        elif \"mistral\" in model_key or \"ministral\" in model_key:\n",
    "            prompt_clean = re.sub(r\"<s>|\\[INST\\]|\\[/INST\\]\", \"\", prompt).strip()\n",
    "            response_clean = re.sub(r\"<s>|\\[INST\\]|\\[/INST\\]\", \"\", response).strip()\n",
    "            return response_clean[len(prompt_clean):].strip() if response_clean.startswith(prompt_clean) else response_clean\n",
    "\n",
    "        elif \"qwen\" in model_key:\n",
    "            match = re.search(r\"assistant\\s*\", response, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                response = response[match.end():].strip()\n",
    "            response = re.sub(r\"^(assistant\\s*)+\", \"\", response, flags=re.IGNORECASE).strip()\n",
    "            return response\n",
    "\n",
    "        elif \"gemma\" in model_key:\n",
    "            if \"model\" in response:\n",
    "                parts = response.split(\"model\", 1)\n",
    "                return parts[1].strip() if len(parts) > 1 else response.strip()\n",
    "            return response.strip()\n",
    "\n",
    "        elif \"internlm\" in model_key:\n",
    "            # Keep everything between first <|Bot|>: and second <|User|>: (truncate at second <|User|>:)\n",
    "            if \"<|User|>:\" in response and \"<|Bot|>:\" in response:\n",
    "                first_bot_idx = response.find(\"<|Bot|>:\")\n",
    "                second_user_idx = response.find(\"<|User|>:\", first_bot_idx + 1)\n",
    "                if second_user_idx != -1:\n",
    "                    return response[first_bot_idx + len(\"<|Bot|>:\"):second_user_idx].strip()\n",
    "                else:\n",
    "                    return response[first_bot_idx + len(\"<|Bot|>:\"):].strip()\n",
    "            else:\n",
    "                return response.strip()\n",
    "\n",
    "        elif \"deepseek\" in model_key:\n",
    "            prefix = f\"### Instruction:\\n{prompt}\\n### Response:\"\n",
    "            return response[len(prefix):].strip() if response.startswith(prefix) else response\n",
    "\n",
    "        elif \"glm\" in model_key:\n",
    "            # Remove up to and including the prefix\n",
    "            prefix = f\"[Round 1]\\n\\né—®ï¼š{prompt}\\n\\nç­”ï¼š\"\n",
    "            if response.startswith(prefix):\n",
    "                response = response[len(prefix):].strip()\n",
    "            # Remove everything after [Round 2]\n",
    "            if \"[Round 2]\" in response:\n",
    "                response = response.split(\"[Round 2]\")[0].strip()\n",
    "            return response\n",
    "\n",
    "    if isinstance(prompts, str) and isinstance(responses, str):\n",
    "        return clean_pair(prompts, responses)\n",
    "\n",
    "    if isinstance(prompts, list) and isinstance(responses, list):\n",
    "        return [clean_pair(p, r) for p, r in zip(prompts, responses)]\n",
    "\n",
    "    raise ValueError(\"prompts and responses must both be str or List[str]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264eeca1-4e19-42c7-b954-9921718854db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Single prompt**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021754e3-13e7-429f-890a-19139ba9be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose the model you want to use\n",
    "# MODEL_KEY = \"mistral_7b_v03_instruct\"\n",
    "# tokenizer, model = load_model(MODEL_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a7dd0e-ff3b-4c24-b49d-085c7f33d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# test_prompt = \"Write a story about a dog going to the moon.\"\n",
    "# # compound_prompt = f\"[USER REQUEST]: '{test_prompt}'\\n{strategy_hint}\"\n",
    "# compound_prompt = f\"[USER REQUEST]: '{test_prompt}'\\n{strategy_hint}\"\n",
    "\n",
    "# response = query_model(MODEL_KEY, tokenizer, model, compound_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c9572c-f1f3-48f7-9c73-de5ecbd7660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_prompt)\n",
    "# print(compound_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8abe83-f14e-4e00-9d07-d377d504c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5272b1df-0830-412f-a53e-1a0ea50c6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_response = clean_response(MODEL_KEY, compound_prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6609c24-6963-48f2-ba1a-f6d03ecd6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cleaned_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1dbd5-122a-41d0-ad3b-14040b259b55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Batch Prompts**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db72de-9c62-4f50-b9be-9182a89825b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKPOINT_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b05c67-2aea-4569-8b03-0960c0d4176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model you want to use\n",
    "PROMPING_MODEL_KEY = \"llama3_8b_instruct\"\n",
    "prompting_tokenizer, prompting_model = load_model(PROMPING_MODEL_KEY)\n",
    "\n",
    "MARKING_MODEL_KEY = \"llama3_8b_instruct\"\n",
    "marking_tokenizer, marking_model = load_model(MARKING_MODEL_KEY)\n",
    "\n",
    "# fine_tuned_path = f\"tuned_models/fine_tuned_{MARKING_MODEL_KEY}\"\n",
    "# marking_tokenizer, marking_model = load_model(MARKING_MODEL_KEY, fine_tuned_path)\n",
    "\n",
    "# TEACHER_MODEL_KEY = \"deepseek_llm_chat\"\n",
    "# distilled_path = f\"tuned_models/distilled_{TEACHER_MODEL_KEY}_to_{MARKING_MODEL_KEY}\"\n",
    "# marking_tokenizer, marking_model = load_model(MARKING_MODEL_KEY, distilled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94f5d2-50a2-4c8b-be07-41be43b60ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single batch from train_loader\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    prompts = batch['prompt']  # Get the prompts from the batch\n",
    "    categories = batch['category']  # Get the category from the batch\n",
    "\n",
    "    prompts_for_promptingLLM = [f\"[USER REQUEST]: '{p}'\\n{strategy_hint}\" for p in prompts]\n",
    "    system_instructions = query_model(PROMPING_MODEL_KEY, prompting_tokenizer, prompting_model, prompts_for_promptingLLM)\n",
    "    system_instructions = [clean_response(PROMPING_MODEL_KEY, p, s) for p, s in zip(prompts_for_promptingLLM, system_instructions)]\n",
    "    \n",
    "    prompts_for_markingLLM_nowatermark = [f\"{p}\\nThe response must be a paragraph under 100 words.\" for p in prompts]\n",
    "    non_watermarked_responses = query_model(MARKING_MODEL_KEY, marking_tokenizer, marking_model, prompts_for_markingLLM_nowatermark)\n",
    "    \n",
    "    prompts_for_markingLLM_watermark = [f\"[USER REQUEST]: {p}\\n[SYSTEM INSTRUCTION]: {s}\\nThe response must be a paragraph under 100 words.\" for p, s in zip(prompts, system_instructions)]\n",
    "    watermarked_responses = query_model(MARKING_MODEL_KEY, marking_tokenizer, marking_model, prompts_for_markingLLM_watermark)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf947543-60ec-4fd6-87e5-31733213fcec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_non_watermarked_responses = [clean_response(MARKING_MODEL_KEY, p, r) for p, r in zip(prompts_for_markingLLM_nowatermark, non_watermarked_responses)]\n",
    "cleaned_watermarked_responses = [clean_response(MARKING_MODEL_KEY, p, r) for p, r in zip(prompts_for_markingLLM_watermark, watermarked_responses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15b34d-9eae-4bbd-b4e5-dc04f0417cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[USER REQUESTS]:\\n----------------------------------\\n{prompts}\\n\")\n",
    "print(f\"[INPUTS FOR PROMPTING_LLM]:\\n----------------------------------\\n{prompts_for_promptingLLM}\\n\")\n",
    "print(f\"[SYSTEM INSTRUCTIONS]:\\n----------------------------------\\n{system_instructions}\\n\")\n",
    "\n",
    "print(f\"[INPUTS FOR MARKING_LLM] (WATERMARK):\\n----------------------------------\\n{prompts_for_markingLLM_watermark}\\n\")\n",
    "print(f\"[WATERMARKED RESPONSES]:\\n----------------------------------\\n{watermarked_responses}\\n\")\n",
    "print(f\"[WATERMARKED RESPONSES] (cleaned):\\n----------------------------------\\n{cleaned_watermarked_responses}\\n\")\n",
    "\n",
    "print(f\"[INPUTS FOR MARKING_LLM (NO WATERMARK)]:\\n----------------------------------\\n{prompts_for_markingLLM_nowatermark}\\n\")\n",
    "print(f\"[NON WATERMARKED RESPONSES]:\\n----------------------------------\\n{non_watermarked_responses}\")\n",
    "print(f\"[NON WATERMARKED RESPONSES] (cleaned):\\n----------------------------------\\n{cleaned_non_watermarked_responses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb46742-e24c-4497-8db4-e5ce141273a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('unclean ----------------')\n",
    "print(watermarked_responses[1])\n",
    "print('cleaned ----------------')\n",
    "print(cleaned_watermarked_responses[1])\n",
    "print('unclean ----------------')\n",
    "print(non_watermarked_responses[1])\n",
    "print('cleaned ----------------')\n",
    "print(cleaned_non_watermarked_responses[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe27930-35cc-47c4-8a4e-36ab4055249c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Data generation**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fffa74-8a85-49cd-ad58-fcc1a276c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKPOINT_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b1be5-462b-4a40-8d3d-356a7f192f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model you want to use\n",
    "PROMPING_MODEL_KEY = \"mistral_7b_v03_instruct\"\n",
    "prompting_tokenizer, prompting_model = load_model(PROMPING_MODEL_KEY)\n",
    "\n",
    "MARKING_MODEL_KEY = \"mistral_7b_v03_instruct\"\n",
    "marking_tokenizer, marking_model = load_model(MARKING_MODEL_KEY)\n",
    "\n",
    "# fine_tuned_path = f\"tuned_models/fine_tuned_{MARKING_MODEL_KEY}\"\n",
    "# marking_tokenizer, marking_model = load_model(MARKING_MODEL_KEY, fine_tuned_path)\n",
    "\n",
    "# TEACHER_MODEL_KEY = \"deepseek_llm_chat\"\n",
    "# distilled_path = f\"tuned_models/distilled_{TEACHER_MODEL_KEY}_to_{MARKING_MODEL_KEY}\"\n",
    "# marking_tokenizer, marking_model = load_model(MARKING_MODEL_KEY, distilled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b767f-1dca-41cb-9f80-202f0e0ea9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the output directory\n",
    "output_directory = f\"Datasets/1k/PLM_{PROMPING_MODEL_KEY}/{MARKING_MODEL_KEY}_data\"\n",
    "# output_directory = f\"Datasets/1k/PLM_{PROMPING_MODEL_KEY}/fine_tuned_{MARKING_MODEL_KEY}_data\"\n",
    "# output_directory = f\"Datasets/1k/PLM_{PROMPING_MODEL_KEY}/distilled_{TEACHER_MODEL_KEY}_to_{MARKING_MODEL_KEY}_data\"\n",
    "\n",
    "if test_ablation:\n",
    "    print(\"Testing on 300\")\n",
    "elif USING_MAX_DATASET:\n",
    "    print(\"20k dataset woo!\")\n",
    "    output_directory = f\"Datasets/10k/PLM_{PROMPING_MODEL_KEY}/{MARKING_MODEL_KEY}_data\"\n",
    "    # output_directory = f\"Datasets/10k/PLM_{PROMPING_MODEL_KEY}/fine_tuned_{MARKING_MODEL_KEY}_data\"\n",
    "    # output_directory = f\"Datasets/10k/PLM_{PROMPING_MODEL_KEY}/distilled_{TEACHER_MODEL_KEY}_to_{MARKING_MODEL_KEY}_data\"\n",
    "else:\n",
    "    print(\"2k dataset smh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778aa42-e181-45bb-af12-cde5348e50bb",
   "metadata": {},
   "source": [
    "**Special case for testing**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44133d93-f67c-4f2c-84b8-ee7d1dc0c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_ablation:\n",
    "    output_directory = f\"ablation/PLM_{PROMPING_MODEL_KEY}/{MARKING_MODEL_KEY}_data/{experiment}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f994f3c-fb00-4f9a-a40e-b67ab7abc071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the directory exists, and create it if it doesn't\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Helper function to create an empty CSV file with headers if it doesn't exist\n",
    "def initialize_csv(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        # Create a new DataFrame with headers if the file doesn't exist and save it as a CSV file\n",
    "        df = pd.DataFrame(columns=[\"CATEGORY\", \"USER REQUEST\", \"SYSTEM INSTRUCTION\", \"WATERMARKED RESPONSE\", \"NON-WATERMARKED RESPONSE\"])\n",
    "        df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fb8cb-363c-48a3-ab96-12a7c2a5ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_requests(dataloader, prompting_tokenizer, prompting_model, marking_tokenizer, marking_model, start_file_index=1, sub_req=100):\n",
    "    \"\"\"\n",
    "    Processes batches of requests and saves the responses in CSV files.\n",
    "\n",
    "    Args:\n",
    "        dataloader: The data loader for batched inputs.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        model: The loaded language model.\n",
    "        start_file_index: The starting index for output CSV files.\n",
    "        start_request_index: The request index to resume processing.\n",
    "        sub_req: The number of requests to save before switching to a new CSV file.\n",
    "    \"\"\"\n",
    "    start_request_index = (start_file_index - 1) * sub_req\n",
    "    count = 0\n",
    "    file_index = start_file_index\n",
    "    data_accumulator = []\n",
    "\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        if idx * batch_size < start_request_index:\n",
    "            print(idx * batch_size, start_request_index)\n",
    "            continue\n",
    "\n",
    "        # Extract batch data\n",
    "        prompts = batch['prompt']  # Get batch of prompts\n",
    "        categories = batch['category']  # Get batch of categories\n",
    "\n",
    "        # **Step 1: Generate System Instructions**\n",
    "        prompts_for_promptingLLM = [f\"[USER REQUEST]: '{p}'\\n{strategy_hint}\" for p in prompts]\n",
    "        system_instructions = query_model(PROMPING_MODEL_KEY, prompting_tokenizer, prompting_model, prompts_for_promptingLLM)\n",
    "        system_instructions = [clean_response(PROMPING_MODEL_KEY, p, s) for p, s in zip(prompts_for_promptingLLM, system_instructions)]\n",
    "\n",
    "        # **Step 2: Generate Non-Watermarked Responses**\n",
    "        prompts_for_markingLLM_nowatermark = [f\"{p}\\nThe response must be a paragraph under 100 words.\" for p in prompts]\n",
    "        non_watermarked_responses = query_model(MARKING_MODEL_KEY, marking_tokenizer, marking_model, prompts_for_markingLLM_nowatermark)\n",
    "        non_watermarked_responses = [clean_response(MARKING_MODEL_KEY, p, r) for p, r in zip(prompts_for_markingLLM_nowatermark, non_watermarked_responses)]\n",
    "\n",
    "        # **Step 3: Generate Watermarked Responses**\n",
    "        prompts_for_markingLLM_watermark = [f\"[USER REQUEST]: {p}\\n[SYSTEM INSTRUCTION]: {s}\\nThe response must be a paragraph under 100 words.\" for p, s in zip(prompts, system_instructions)]\n",
    "        watermarked_responses = query_model(MARKING_MODEL_KEY, marking_tokenizer, marking_model, prompts_for_markingLLM_watermark)\n",
    "        watermarked_responses = [clean_response(MARKING_MODEL_KEY, p, r) for p, r in zip(prompts_for_markingLLM_watermark, watermarked_responses)]\n",
    "\n",
    "        # **Step 4: Append data to the accumulator**\n",
    "        for category, prompt, system_inst, wm_response, nwm_response in zip(categories, prompts, system_instructions, watermarked_responses, non_watermarked_responses):\n",
    "            data_accumulator.append({\n",
    "                \"CATEGORY\": category,\n",
    "                \"USER REQUEST\": prompt,\n",
    "                \"SYSTEM INSTRUCTION\": system_inst,\n",
    "                \"WATERMARKED RESPONSE\": wm_response,\n",
    "                \"NON-WATERMARKED RESPONSE\": nwm_response\n",
    "            })\n",
    "            # print(f\"{count + start_request_index}. {category}: {prompt}\")\n",
    "\n",
    "            count += 1\n",
    "            print(f\"Processed {count + start_request_index} user requests.\")\n",
    "\n",
    "        # **Step 5: Save Data in Batches to CSV**\n",
    "        if count % sub_req == 0:\n",
    "            output_file_path = os.path.join(output_directory, f\"{file_index}.csv\")\n",
    "\n",
    "            # Initialize CSV if not exists\n",
    "            initialize_csv(output_file_path)\n",
    "\n",
    "            # Save accumulated data\n",
    "            pd.DataFrame(data_accumulator).to_csv(output_file_path, mode='a', index=False, header=False)\n",
    "            print(f\"Saved {output_file_path} with {count + start_request_index} requests.\")\n",
    "\n",
    "            # Clear accumulator and increment file index\n",
    "            data_accumulator = []\n",
    "            file_index += 1\n",
    "\n",
    "    # **Step 6: Save any remaining data**\n",
    "    if data_accumulator:\n",
    "        output_file_path = os.path.join(output_directory, f\"{file_index}.csv\")\n",
    "        initialize_csv(output_file_path)\n",
    "        pd.DataFrame(data_accumulator).to_csv(output_file_path, mode='a', index=False, header=False)\n",
    "        print(f\"Saved {output_file_path} with remaining {len(data_accumulator)} requests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf28e1-708b-44bf-b263-38f03f86ac64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "process_requests(dataloader, prompting_tokenizer, prompting_model, marking_tokenizer, marking_model, start_file_index=1, sub_req=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cent",
   "language": "python",
   "name": "torch_cent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
